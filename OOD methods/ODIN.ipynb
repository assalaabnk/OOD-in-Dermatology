{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI2o5jNVMyif"
   },
   "source": [
    "Resources: https://github.com/facebookresearch/odin &\n",
    "https://github.com/mattgroh/fitzpatrick17k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Erdgp8sEokt2",
    "outputId": "ffb9139a-91f1-4475-f2d1-b6ed21d7d943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.8.1.78)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tensorflow keras opencv-python-headless scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q74UOSC0mUwS"
   },
   "outputs": [],
   "source": [
    "#!pip install numpy tensorflow==2.8.0 keras opencv-python-headless scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fvi64OE2gcis",
    "outputId": "36917350-98ea-4c13-ed04-f9a30cb32e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlD0veiJpBnd"
   },
   "source": [
    "Dealing with compability problem :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLDjXMVFo6f0",
    "outputId": "c2aaf081-d236-49e6-ac07-c8246f3d7c7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.Session(config=tf.ConfigProto( log_device_placement=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_Rr1-ACLuPb"
   },
   "source": [
    "## Fitz dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st9ggAgCv_UC"
   },
   "source": [
    "### DenseNET implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIZp43R6vVH9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge2w_bqUOpcB"
   },
   "source": [
    "**Loading the pretrained DenseNet model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P71ii-fmZB9Z",
    "outputId": "904c9e32-ccd9-4cb3-e6eb-92e9ba7a5e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/init_ops.py:94: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/init_ops.py:94: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/init_ops.py:94: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    }
   ],
   "source": [
    "print ('Loading Network...')\n",
    "model_path = '/content/Models/my_model_densenet.h5'\n",
    "densenet_model  = tf.keras.models.load_model(model_path)\n",
    "print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYQ4a09ZmhVu"
   },
   "source": [
    "# Image generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "DF-v0OT6pKG8",
    "outputId": "75287279-b935-4dd2-ce95-8293715fd631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "argparse"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLiQGIHEo_x7",
    "outputId": "a18d624c-b4bc-42aa-a796-98c2ba1ee839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using the following parameters:\n",
      "augment: True\n",
      "batch_size: 40\n",
      "checkpoint_frequency: 1000\n",
      "dataset_root: /usr/xtmp/hannah/segkeras/nn-isic2019/dataset/\n",
      "decay_start_iteration: 15000\n",
      "detailed_logs: False\n",
      "early_stop_patience: 8\n",
      "experiment_root: FL_Aug_MEL\n",
      "gpu_device: 0\n",
      "heavy_augment: False\n",
      "init_epoch: 0\n",
      "initial_checkpoint: None\n",
      "is_train: 1\n",
      "learning_rate: 1e-06\n",
      "load_weights: None\n",
      "loading_threads: 8\n",
      "loss_type: 1\n",
      "margin: soft\n",
      "num_classes: 7\n",
      "num_epochs: 25\n",
      "num_gpus: 1\n",
      "resume: False\n",
      "sizeH: 224\n",
      "sizeW: 224\n",
      "skip_class: MEL\n",
      "train_iterations: 25000\n"
     ]
    }
   ],
   "source": [
    "from argument_parser import myParser\n",
    "args = myParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0N-pc_JuKAi",
    "outputId": "71343565-783d-4ff1-f0bf-cf4d3a88a391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Generators...\n",
      "Loading Testing Dataset...\n",
      "   Done.\n"
     ]
    }
   ],
   "source": [
    "#Fitz Darker only\n",
    "from data_generators import fitzpatrick17kImageDataset\n",
    "## Loading Data Generators\n",
    "ROOT = '/content/Datasets/dataset/fitzpatrick 17k /'\n",
    "\n",
    "print('Loading Data Generators...')\n",
    "#args.is_train =  # Training Set\n",
    "args.num_classes = 3\n",
    "args.sizeW = 224\n",
    "args.sizeH = 224\n",
    "args.dataset_root = ROOT\n",
    "args.is_train = 2 # Validation Set\n",
    "Testing_generator_darker_only = fitzpatrick17kImageDataset(args)\n",
    "# args.is_train = 1 # Validation Set\n",
    "# train_generator = ISICImageDataset(args)\n",
    "print('   Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgsqykNOnhKr",
    "outputId": "9fbe0225-2ecf-49bb-c5b9-64580abfae39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Generators...\n",
      "Loading Testing Dataset...\n",
      "   Done.\n"
     ]
    }
   ],
   "source": [
    "#Fitz Lighter only\n",
    "from data_generators import fitzpatrick17kImageDataset\n",
    "## Loading Data Generators\n",
    "ROOT = '/content/Datasets/dataset/fitzpatrick 17k /'\n",
    "\n",
    "print('Loading Data Generators...')\n",
    "#args.is_train =  # Training Set\n",
    "args.num_classes = 3\n",
    "args.sizeW = 224\n",
    "args.sizeH = 224\n",
    "args.dataset_root = ROOT\n",
    "args.is_train = 2 # Validation Set  change the condition on common.py before running\n",
    "Testing_generator_lighter_only = fitzpatrick17kImageDataset(args)\n",
    "# args.is_train = 1 # Validation Set\n",
    "# train_generator = ISICImageDataset(args)\n",
    "print('   Done.')\n",
    "\n",
    "#/content/drive/MyDrive/Assala Master Project/Datasets/dataset/fitzpatrick 17k /Darker_Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLQGdhPsn2n2",
    "outputId": "5e09e531-f89e-4df3-cc29-85fe4d453a7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Generators...\n",
      "Loading Validation Dataset...\n",
      "   Done.\n"
     ]
    }
   ],
   "source": [
    "#Fitz ALL images\n",
    "from data_generators import fitzpatrick17kImageDataset\n",
    "## Loading Data Generators\n",
    "ROOT = '/content/Datasets/dataset/fitzpatrick 17k /'\n",
    "\n",
    "print('Loading Data Generators...')\n",
    "#args.is_train =  # Training Set\n",
    "args.num_classes = 3\n",
    "args.sizeW = 224\n",
    "args.sizeH = 224\n",
    "args.dataset_root = ROOT\n",
    "args.is_train = 0 # Validation Set\n",
    "Testing_generator_ALL = fitzpatrick17kImageDataset(args)\n",
    "# args.is_train = 1 # Validation Set\n",
    "# train_generator = ISICImageDataset(args)\n",
    "print('   Done.')\n",
    "\n",
    "#/content/drive/MyDrive/Assala Master Project/Datasets/dataset/fitzpatrick 17k /Darker_Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEZd-bpRa0yB",
    "outputId": "8c821927-1ed2-413c-ad66-6b96110870bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Generators...\n",
      "class_names:['MEL', 'NV', 'BCC', 'AK', 'BKL', 'SCC']\n",
      "skip_class:['DF', 'VASC']\n",
      "Loading Testing Dataset...\n",
      "   Done.\n"
     ]
    }
   ],
   "source": [
    "#ISIC\n",
    "from data_generators import ISICImageDataset\n",
    "## Loading Data Generators\n",
    "ROOT = '/content/Datasets/ISIC2019/ISIC_2019_Training_Input/'\n",
    "\n",
    "print('Loading Data Generators...')\n",
    "args.is_train = 2 # Training Set\n",
    "args.num_classes = 6\n",
    "args.sizeW = 224\n",
    "args.sizeH = 224\n",
    "args.dataset_root = ROOT\n",
    "args.is_train = 2 # Validation Set\n",
    "Validation_generator = ISICImageDataset(args)\n",
    "# args.is_train = 1 # Validation Set\n",
    "# train_generator = ISICImageDataset(args)\n",
    "print('   Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yuFqH0OvIFp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def Compute_baseline_softmax_scores(model, generator_in, generator_out):\n",
    "    \"\"\"\n",
    "    Calculate the base confidence of the output, no perturbation added here, no temperature scaling used.\n",
    "    Directly copy the original prediction results.\n",
    "    \"\"\"\n",
    "    print('Begin to compute baseline softmax scores')\n",
    "    distributions = ['In', 'Out']\n",
    "\n",
    "    for dist in distributions:\n",
    "        if dist == 'In':\n",
    "            generator = generator_in\n",
    "        elif dist == 'Out':\n",
    "            generator = generator_out\n",
    "\n",
    "        with open(\"/content/ODIN_denseNet_{}.txt\".format(dist), 'w') as f:\n",
    "            # Access file paths from the generator\n",
    "            file_paths = generator.filepaths\n",
    "            for i in range(len(file_paths)):\n",
    "                thisImg = Image.open(file_paths[i]).convert(\"RGB\")\n",
    "                thisImg = np.expand_dims(np.asarray(thisImg.resize((224,224)))/255, axis=0)\n",
    "                softmax_probs = model.predict(thisImg)\n",
    "                softmax_score = np.max(softmax_probs)\n",
    "                f.write(\"{}\\n\".format(softmax_score))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwgcrcDHCL4V"
   },
   "outputs": [],
   "source": [
    "def compute_baseline_softmax_scores(model, generator_in, generator_out):\n",
    "    \"\"\"\n",
    "    Calculate the base confidence of the output, no perturbation added here, no temperature scaling used.\n",
    "    Directly copy the original prediction results.\n",
    "    \"\"\"\n",
    "    print('Begin to compute baseline softmax scores')\n",
    "    distributions = ['In', 'Out']\n",
    "\n",
    "\n",
    "    for dist in distributions:\n",
    "        if dist == 'In':\n",
    "            generator = generator_in\n",
    "        elif dist == 'Out':\n",
    "            generator = generator_out\n",
    "\n",
    "        with open(\"./densenet121_Base_{}.txt\".format(dist), 'w') as f:\n",
    "            for i in range(len(generator.fids)):\n",
    "                # print('thisFid: '+str(generator.fids[i]))\n",
    "                thisImg = Image.open(generator.fids[i]+'.jpg').convert(\"RGB\")\n",
    "                thisImg = np.expand_dims(np.asarray(thisImg.resize((224,224)))/255, axis =0)\n",
    "                softmax_probs = model.predict(thisImg)\n",
    "                softmax_score = np.max(softmax_probs)\n",
    "                f.write(\"{}\\n\".format(softmax_score))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCNBpl5TbBIj"
   },
   "source": [
    "the perturbation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF1g8NvqxKJe"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def get_perturbation_helper_func(model, temperature, num_classes):\n",
    "    \"\"\" Return Keras functions for calculating perturbations. \"\"\"\n",
    "    # Compute loss based on the second last layer's output and temperature scaling\n",
    "    dense_pred_layer_output = model.get_layer('dense_4').output\n",
    "    scaled_dense_pred_output = dense_pred_layer_output / temperature\n",
    "\n",
    "    print(dense_pred_layer_output)\n",
    "    print(scaled_dense_pred_output)\n",
    "\n",
    "    print(K.argmax(model.outputs))\n",
    "\n",
    "    label_tensor = K.one_hot(K.argmax(model.outputs), 1)\n",
    "    tf.print(label_tensor)\n",
    "    loss = K.sparse_categorical_crossentropy(label_tensor, scaled_dense_pred_output, from_logits=True)\n",
    "    grad_loss = K.gradients(loss, model.inputs)\n",
    "\n",
    "    # The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "    compute_perturbations = K.function(model.inputs + [K.learning_phase()], grad_loss)\n",
    "\n",
    "    # https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer\n",
    "    get_scaled_dense_pred_output = K.function(model.inputs + [K.learning_phase()], [scaled_dense_pred_output])\n",
    "\n",
    "    return compute_perturbations, get_scaled_dense_pred_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU_pImXqY91M"
   },
   "source": [
    "# Find the best parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO6-96wbdrfw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from odin import norm_perturbations\n",
    "from scipy.special import softmax\n",
    "\n",
    "def compute_odin_softmax_scores(model, generator_in, generator_out, num_classes = 6, batch_size = 1):\n",
    "    distributions = ['In', 'Out']\n",
    "\n",
    "    # This file is used for recording what parameter combinations were already computed.\n",
    "    progress_file = os.path.join('Done.txt')\n",
    "    done_set = set()\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            done_set = set(line.rstrip('\\n') for line in f)\n",
    "\n",
    "    # ODIN parameters\n",
    "\n",
    "    model_name = 'DenseNet201'\n",
    "    #Grid Search\n",
    "    temperatures = [1000, 500, 200, 100, 50, 20, 10, 5, 2, 1]\n",
    "    \n",
    "    magnitudes = np.round(np.arange(0, 0.0041, 0.0002), 4)\n",
    "\n",
    "    need_norm_perturbations = 1\n",
    "\n",
    "\n",
    "\n",
    "#     model_param_map = get_transfer_model_param_map()\n",
    "    image_data_format = K.image_data_format()\n",
    "    learning_phase = 0 # 0 = test, 1 = train\n",
    "\n",
    "\n",
    "\n",
    "    for temperature in temperatures:\n",
    "        compute_perturbations, get_scaled_dense_pred_output = get_perturbation_helper_func(model, temperature, num_classes)\n",
    "\n",
    "        for magnitude in magnitudes:\n",
    "            for dist in distributions:\n",
    "                # Skip if the parameter combination has done\n",
    "                param_comb_id = \"{}, {}, {}\".format( dist, temperature, magnitude)\n",
    "                if param_comb_id in done_set:\n",
    "                    print('Skip ', param_comb_id)\n",
    "                    continue\n",
    "\n",
    "                if dist == 'In':\n",
    "                    generator = generator_in\n",
    "                elif dist == 'Out':\n",
    "                    generator = generator_out\n",
    "\n",
    "                print(\"\\n===== Temperature: {}, Magnitude: {}, {}-Distribution =====\".format(temperature, magnitude, dist))\n",
    "\n",
    "                try:\n",
    "                    f = open(\"./densenet121_ODIN_{}_{}_{}.txt\".format(temperature, magnitude,dist), 'w')\n",
    "                    for i in trange(len(generator.fids)):\n",
    "                        thisFid = generator.fids[i]\n",
    "                        images = np.expand_dims(cv2.resize(np.array(Image.open(thisFid+'.jpg').convert(\"RGB\"))/255, (224,224), interpolation=cv2.INTER_LINEAR), axis = 0)\n",
    "\n",
    "                        perturbations = compute_perturbations([images, learning_phase])[0]\n",
    "                    # Get sign of perturbations\n",
    "                        perturbations = np.sign(perturbations)\n",
    "\n",
    "                    # Normalize the perturbations to the same space of image\n",
    "                    # https://github.com/facebookresearch/odin/issues/5\n",
    "                    # Perturbations divided by ISIC Training Set STD\n",
    "                        if need_norm_perturbations:\n",
    "                            perturbations = norm_perturbations(perturbations, image_data_format)\n",
    "\n",
    "                    # Add perturbations to images\n",
    "                        perturbative_images = images - magnitude * perturbations\n",
    "\n",
    "                    # Calculate the confidence after adding perturbations\n",
    "                        dense_pred_outputs = get_scaled_dense_pred_output([perturbative_images, learning_phase])[0]\n",
    "                        softmax_probs = softmax(dense_pred_outputs)\n",
    "                        softmax_scores = np.max(softmax_probs, axis=-1)\n",
    "\n",
    "                        for s in softmax_scores:\n",
    "                            f.write(\"{}\\n\".format(s))\n",
    "                    f.close()\n",
    "\n",
    "                    with open(progress_file, 'a') as f_done:\n",
    "                        f_done.write(\"{}\\n\".format(param_comb_id))\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    # Handle the \"File not found\" error\n",
    "                    print(f\"File not found for {param_comb_id}. Skipping...\")\n",
    "                    continue\n",
    "                last_processed_combination = param_comb_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiCMEp-45Q93"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from odin import norm_perturbations\n",
    "from scipy.special import softmax\n",
    "\n",
    "def compute_odin_softmax_scores(model, generator_in, generator_out, num_classes=6, batch_size=1):\n",
    "    distributions = ['In', 'Out']\n",
    "\n",
    "    # This file is used for recording what parameter combinations were already computed.\n",
    "    progress_file = os.path.join('Done.txt')\n",
    "    done_set = set()\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            done_set = set(line.rstrip('\\n') for line in f)\n",
    "\n",
    "    # ODIN parameters\n",
    "    model_name = 'DenseNet201'\n",
    "    temperatures = [1000, 500, 200, 100, 50, 20, 10, 5, 2, 1]\n",
    "    #temperatures 200\n",
    "\n",
    "    magnitudes = np.round(np.arange(0.0002, 0.0041, 0.0002), 4)\n",
    "    #magnitudes = np.round([0.0002, 0.0041], 4)\n",
    "    need_norm_perturbations = 1\n",
    "    image_data_format = K.image_data_format()\n",
    "    learning_phase = 0  # 0 = test, 1 = train\n",
    "\n",
    "    for temperature in temperatures:\n",
    "        compute_perturbations, get_scaled_dense_pred_output = get_perturbation_helper_func(model, temperature, num_classes)\n",
    "\n",
    "        for magnitude in magnitudes:\n",
    "            for dist in distributions:\n",
    "                # Skip if the parameter combination has done\n",
    "                param_comb_id = \"{}, {}, {}\".format(dist, temperature, magnitude)\n",
    "                if param_comb_id in done_set:\n",
    "                    print('Skip ', param_comb_id)\n",
    "                    continue\n",
    "\n",
    "                if dist == 'In':\n",
    "                    generator = generator_in\n",
    "                elif dist == 'Out':\n",
    "                    generator = generator_out\n",
    "\n",
    "                print(\"\\n===== Temperature: {}, Magnitude: {}, {}-Distribution =====\".format(temperature, magnitude, dist))\n",
    "\n",
    "                # Check if the output file already exists, and if it does, skip processing\n",
    "                output_file_path = \"./densenet121_ODIN_{}_{}_{}.txt\".format(temperature, magnitude, dist)\n",
    "                if os.path.exists(output_file_path):\n",
    "                    print(f\"Output file {output_file_path} already exists. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    f = open(output_file_path, 'w')\n",
    "                    for i in trange(len(generator.fids)):\n",
    "                        thisFid = generator.fids[i]\n",
    "                        images = np.expand_dims(cv2.resize(np.array(Image.open(thisFid + '.jpg').convert(\"RGB\")) / 255,\n",
    "                                                           (224, 224), interpolation=cv2.INTER_LINEAR), axis=0)\n",
    "\n",
    "                        perturbations = compute_perturbations([images, learning_phase])[0]\n",
    "                        perturbations = np.sign(perturbations)\n",
    "\n",
    "                        if need_norm_perturbations:\n",
    "                            perturbations = norm_perturbations(perturbations, image_data_format)\n",
    "\n",
    "                        perturbative_images = images - magnitude * perturbations\n",
    "\n",
    "                        dense_pred_outputs = get_scaled_dense_pred_output([perturbative_images, learning_phase])[0]\n",
    "                        softmax_probs = softmax(dense_pred_outputs)\n",
    "                        softmax_scores = np.max(softmax_probs, axis=-1)\n",
    "\n",
    "                        for s in softmax_scores:\n",
    "                            f.write(\"{}\\n\".format(s))\n",
    "                    f.close()\n",
    "\n",
    "                    with open(progress_file, 'a') as f_done:\n",
    "                        f_done.write(\"{}\\n\".format(param_comb_id))\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Handle any exceptions that might occur during file processing\n",
    "                    print(f\"An error occurred for {param_comb_id}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                last_processed_combination = param_comb_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo6hTeBnISxW"
   },
   "outputs": [],
   "source": [
    "compute_odin_softmax_scores(densenet_model, Validation_generator, Testing_generator_darker_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_odin_softmax_scores(densenet_model, Validation_generator, Testing_generator_lighter_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_odin_softmax_scores(densenet_model, Validation_generator, Testing_generator_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_odin_softmax_scores(densenet_model, Validation_generator, Testing_generator_darker_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zQEm9jgV-dc"
   },
   "outputs": [],
   "source": [
    "#testing the subset of parameters\n",
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from odin import norm_perturbations\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "need_norm_perturbations = 1\n",
    "best_auroc = 0.0\n",
    "best_auroc_params = None\n",
    "lowest_fpr = float('inf')\n",
    "lowest_fpr_params = None\n",
    "\n",
    "\n",
    "#We want to loop over all the saved scores\n",
    "In_scores_files =\"/Models/ODIN/Saved scores/In\"\n",
    "Out_scores_files = \"/Models/ODIN/Saved scores/Out\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in_dist_file=\"densenet121_ODIN_{}_{}_In.txt\".format(temperature, magnitude)\n",
    "out_dist_file=\"densenet121_ODIN_{}_{}_Out.txt\".format(temperature, magnitude)\n",
    "\n",
    "\n",
    "\n",
    " # Calculate AUROC\n",
    "auroc_value = auroc(in_dist_file=  , out_dist_file=out_dist_file)\n",
    "\n",
    "                    # Calculate FPR\n",
    "tpr, fpr = get_tpr_and_fpr(scores_in_test, scores_out_test, delta)\n",
    "\n",
    "                    # Update best AUROC and FPR if necessary\n",
    "if auroc_value > best_auroc:\n",
    "   best_auroc = auroc_value\n",
    "   best_auroc_params = (temperature, magnitude, dist)\n",
    "\n",
    "if fpr < lowest_fpr:\n",
    "   lowest_fpr = fpr\n",
    "   lowest_fpr_params = (temperature, magnitude, dist)\n",
    "\n",
    "print(\"Best AUROC: {:.4f}, Parameters: Temperature={}, Magnitude={}, Distribution={}\"\n",
    "      .format(best_auroc, *best_auroc_params))\n",
    "\n",
    "    # Print the lowest FPR and corresponding parameters\n",
    "print(\"Lowest FPR: {:.4f}, Parameters: Temperature={}, Magnitude={}, Distribution={}\"\n",
    "      .format(lowest_fpr, *lowest_fpr_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQNrE1jwuZKO",
    "outputId": "f9e26abf-fb90-40e4-d163-0900c3066e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 6), dtype=float32)\n",
      "Tensor(\"truediv:0\", shape=(?, 6), dtype=float32)\n",
      "Tensor(\"ArgMax:0\", shape=(1, ?), dtype=int64)\n",
      "Skip  In, 1000, 0.0\n",
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [27:31<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0002, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [29:22<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0002, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:27<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0004, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [29:31<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0004, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:33<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0006, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [30:08<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0006, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:52<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0008, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [29:59<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0008, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:45<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.001, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [30:18<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.001, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:37<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0012, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [30:37<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0012, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:46<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0014, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [30:12<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0014, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:34<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Temperature: 1000, Magnitude: 0.0016, In-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 1013/2495 [12:01<18:12,  1.36it/s]"
     ]
    }
   ],
   "source": [
    "compute_odin_softmax_scores(densenet_model,Validation_generator, Testing_generator_darker_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_odin_softmax_scores(densenet_model,Validation_generator, Testing_generator_lighter_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_9G5PkOI9AF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from odin import norm_perturbations\n",
    "from scipy.special import softmax\n",
    "from tqdm import trange\n",
    "\n",
    "# ODIN parameters\n",
    "#temperatures = [1000, 500, 200, 100, 50, 20, 10, 5, 2, 1]\n",
    "#magnitudes = np.round(np.arange(0, 0.0041, 0.0002), 4)\n",
    "optimal_temperature = 200\n",
    "optimal_magnitude = 0.0002\n",
    "optimal_delta = 0.90385\n",
    "\n",
    "\n",
    "def compute_odin_softmax_scores (model, generator_in, generator_out, num_classes=6, batch_size=1):\n",
    "    distributions = ['In', 'Out']\n",
    "\n",
    "    # Specify the desired starting magnitude and temperature\n",
    "    start_magnitude = 0.0008\n",
    "    start_temperature = 1000\n",
    "\n",
    "    # Find the indices for the desired magnitude and temperature\n",
    "    start_magnitude_idx = np.where(magnitudes == start_magnitude)[0][0]\n",
    "    start_temperature_idx = temperatures.index(start_temperature)\n",
    "\n",
    "    # This file is used for recording what parameter combinations were already computed.\n",
    "    progress_file = os.path.join('Done.txt')\n",
    "    done_set = set()\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            done_set = set(line.rstrip('\\n') for line in f)\n",
    "\n",
    "    image_data_format = K.image_data_format()\n",
    "    learning_phase = 0  # 0 = test, 1 = train\n",
    "\n",
    "    need_norm_perturbations = 1\n",
    "\n",
    "    for temperature_idx, temperature in enumerate(temperatures[start_temperature_idx:]):\n",
    "\n",
    "        if temperature_idx == 0:\n",
    "            magnitude_start_idx = start_magnitude_idx\n",
    "        else:\n",
    "            magnitude_start_idx = 0\n",
    "\n",
    "        compute_perturbations, get_scaled_dense_pred_output = get_perturbation_helper_func(model, temperature, num_classes)\n",
    "\n",
    "        for magnitude_idx, magnitude in enumerate(magnitudes[magnitude_start_idx:]):\n",
    "            for dist in distributions:\n",
    "                # Skip if the parameter combination has been done\n",
    "                param_comb_id = \"{}, {}, {}\".format(dist, temperature, magnitude)\n",
    "                if param_comb_id in done_set:\n",
    "                    print('Skip ', param_comb_id)\n",
    "                    continue\n",
    "\n",
    "                if dist == 'In':\n",
    "                    generator = generator_in\n",
    "                elif dist == 'Out':\n",
    "                    generator = generator_out\n",
    "\n",
    "                print(\"\\n===== Temperature: {}, Magnitude: {}, {}-Distribution =====\".format(temperature, magnitude, dist))\n",
    "\n",
    "                f = open(\"./densenet121_ODIN_{}_{}_{}.txt\".format(temperature, magnitude, dist), 'w')\n",
    "\n",
    "                # Start from the beginning\n",
    "                for i in trange(len(generator.fids)):\n",
    "                    try:\n",
    "                        thisFid = generator.fids[i]\n",
    "                        images = np.expand_dims(\n",
    "                            cv2.resize(np.array(Image.open(thisFid + '.jpg').convert(\"RGB\")) / 255, (224, 224),\n",
    "                                       interpolation=cv2.INTER_LINEAR), axis=0)\n",
    "\n",
    "                        perturbations = compute_perturbations([images, learning_phase])[0]\n",
    "                        # Get the sign of perturbations\n",
    "                        perturbations = np.sign(perturbations)\n",
    "\n",
    "                        # Normalize the perturbations to the same space of the image\n",
    "                        if need_norm_perturbations:\n",
    "                            perturbations = norm_perturbations(perturbations, image_data_format)\n",
    "\n",
    "                        # Add perturbations to images\n",
    "                        perturbative_images = images - magnitude * perturbations\n",
    "\n",
    "                        # Calculate the confidence after adding perturbations\n",
    "                        dense_pred_outputs = get_scaled_dense_pred_output([perturbative_images, learning_phase])[0]\n",
    "                        softmax_probs = softmax(dense_pred_outputs)\n",
    "                        softmax_scores = np.max(softmax_probs, axis=-1)\n",
    "                        for s in softmax_scores:\n",
    "                            f.write(\"{}\\n\".format(s))\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found for index {i}. Continuing...\")\n",
    "                        continue\n",
    "\n",
    "                f.close()\n",
    "\n",
    "                with open(progress_file, 'a') as f_done:\n",
    "                    f_done.write(\"{}\\n\".format(param_comb_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql0mrBzDTuTt"
   },
   "source": [
    "## Setting the optimal parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWFXBuxbbANq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from odin import norm_perturbations\n",
    "from odin import get_tpr_and_fpr, auroc\n",
    "from scipy.special import softmax\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Optimal ODIN Parameters\n",
    "optimal_temperature = 200\n",
    "optimal_magnitude = 0.0002\n",
    "optimal_delta = 0.90385\n",
    "\n",
    "def compute_odin_parameters_and_metrics(model, generator_in, generator_out, num_classes=6, batch_size=1):\n",
    "    distributions = ['In', 'Out']\n",
    "    compute_perturbations, get_scaled_dense_pred_output = get_perturbation_helper_func(model, optimal_temperature, num_classes)\n",
    "\n",
    "    # This file is used for recording what parameter combinations were already computed.\n",
    "    progress_file = os.path.join('Done.txt')\n",
    "    done_set = set()\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            done_set = set(line.rstrip('\\n') for line in f)\n",
    "\n",
    "    image_data_format = K.image_data_format()\n",
    "    learning_phase = 0  # 0 = test, 1 = train\n",
    "\n",
    "    need_norm_perturbations = 1\n",
    "\n",
    "    # Initialize variables to store metrics\n",
    "    auroc_max = -1  # Initialize AUROC to a very low value\n",
    "    fpr_min = 1     # Initialize FPR to a very high value\n",
    "    odinparam_auroc_max = None\n",
    "    odinparam_fpr_min = None\n",
    "\n",
    "    for dist in distributions:\n",
    "        for magnitude in [optimal_magnitude]:\n",
    "            # Skip if the parameter combination has been done\n",
    "            param_comb_id = \"{}, {}, {}\".format(dist, optimal_temperature, magnitude)\n",
    "            if param_comb_id in done_set:\n",
    "                print('Skip ', param_comb_id)\n",
    "                continue\n",
    "\n",
    "            if dist == 'In':\n",
    "                generator = generator_in\n",
    "            elif dist == 'Out':\n",
    "                generator = generator_out\n",
    "\n",
    "            print(\"\\n===== Temperature: {}, Magnitude: {}, {}-Distribution =====\".format(optimal_temperature, magnitude, dist))\n",
    "\n",
    "            f = open(\"./densenet121_ODIN_{}_{}_{}.txt\".format(optimal_temperature, magnitude, dist), 'w')\n",
    "\n",
    "            # Start from the beginning\n",
    "            for i in trange(len(generator.fids)):\n",
    "                try:\n",
    "                    thisFid = generator.fids[i]\n",
    "                    images = np.expand_dims(\n",
    "                        cv2.resize(np.array(Image.open(thisFid + '.jpg').convert(\"RGB\")) / 255, (224, 224),\n",
    "                                   interpolation=cv2.INTER_LINEAR), axis=0)\n",
    "\n",
    "                    perturbations = compute_perturbations([images, learning_phase])[0]\n",
    "                    # Get the sign of perturbations\n",
    "                    perturbations = np.sign(perturbations)\n",
    "\n",
    "                    # Normalize the perturbations to the same space of the image\n",
    "                    if need_norm_perturbations:\n",
    "                        perturbations = norm_perturbations(perturbations, image_data_format)\n",
    "\n",
    "                    # Add perturbations to images\n",
    "                    perturbative_images = images - magnitude * perturbations\n",
    "\n",
    "                    # Calculate the confidence after adding perturbations\n",
    "                    dense_pred_outputs = get_scaled_dense_pred_output([perturbative_images, learning_phase])[0]\n",
    "                    softmax_probs = softmax(dense_pred_outputs)\n",
    "                    softmax_scores = np.max(softmax_probs, axis=-1)\n",
    "                    for s in softmax_scores:\n",
    "                        f.write(\"{}\\n\".format(s))\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found for index {i}. Continuing...\")\n",
    "                    continue\n",
    "\n",
    "            f.close()\n",
    "\n",
    "            with open(progress_file, 'a') as f_done:\n",
    "                f_done.write(\"{}\\n\".format(param_comb_id))\n",
    "\n",
    "\n",
    "# Call the function to compute ODIN parameters and metrics\n",
    "#compute_odin_parameters_and_metrics(densenet_model, Validation_generator, Testing_generator_lighter_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_FSBBS6upRo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from odin import norm_perturbations\n",
    "from scipy.special import softmax\n",
    "from tqdm import trange\n",
    "\n",
    "# Fixed ODIN Parameters\n",
    "#optimal_temperature = 2\n",
    "#optimal_magnitude = 0.0002\n",
    "\n",
    "optimal_temperature = 200\n",
    "optimal_magnitude = 0.0002\n",
    "\n",
    "def compute_odin_softmax_scores_fixed(model, generator_in, generator_out, num_classes=6, batch_size=1):\n",
    "    distributions = ['Out']\n",
    "    compute_perturbations, get_scaled_dense_pred_output = get_perturbation_helper_func(model, optimal_temperature, num_classes)\n",
    "\n",
    "    # This file is used for recording what parameter combinations were already computed.\n",
    "    progress_file = os.path.join('Done.txt')\n",
    "    done_set = set()\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            done_set = set(line.rstrip('\\n') for line in f)\n",
    "\n",
    "    image_data_format = K.image_data_format()\n",
    "    learning_phase = 0  # 0 = test, 1 = train\n",
    "    need_norm_perturbations = 1\n",
    "\n",
    "    for dist in distributions:\n",
    "        param_comb_id = \"{}, {}, {}\".format(dist, optimal_temperature, optimal_magnitude)\n",
    "        if param_comb_id in done_set:\n",
    "            print('Skip ', param_comb_id)\n",
    "            continue\n",
    "\n",
    "        if dist == 'In':\n",
    "            generator = generator_in\n",
    "        elif dist == 'Out':\n",
    "            generator = generator_out\n",
    "\n",
    "        print(\"\\n===== Temperature: {}, Magnitude: {}, {}-Distribution =====\".format(optimal_temperature, optimal_magnitude, dist))\n",
    "\n",
    "        f = open(\"./densenet121_ODIN_{}_{}_{}.txt\".format(optimal_temperature, optimal_magnitude, dist), 'w')\n",
    "\n",
    "        for i in trange(len(generator.fids)):\n",
    "            try:\n",
    "                thisFid = generator.fids[i]\n",
    "                images = np.expand_dims(\n",
    "                    cv2.resize(np.array(Image.open(thisFid + '.jpg').convert(\"RGB\")) / 255, (224, 224),\n",
    "                               interpolation=cv2.INTER_LINEAR), axis=0)\n",
    "\n",
    "                perturbations = compute_perturbations([images, learning_phase])[0]\n",
    "                # Get sign of perturbations\n",
    "                perturbations = np.sign(perturbations)\n",
    "\n",
    "                # Normalize the perturbations to the same space of image\n",
    "                if need_norm_perturbations:\n",
    "                    perturbations = norm_perturbations(perturbations, image_data_format)\n",
    "\n",
    "                # Add perturbations to images\n",
    "                perturbative_images = images - optimal_magnitude * perturbations\n",
    "\n",
    "                # Calculate the confidence after adding perturbations\n",
    "                dense_pred_outputs = get_scaled_dense_pred_output([perturbative_images, learning_phase])[0]\n",
    "                softmax_probs = softmax(dense_pred_outputs)\n",
    "                softmax_scores = np.max(softmax_probs, axis=-1)\n",
    "                for s in softmax_scores:\n",
    "                    f.write(\"{}\\n\".format(s))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found for index {i}. Continuing...\")\n",
    "                continue\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        with open(progress_file, 'a') as f_done:\n",
    "            f_done.write(\"{}\\n\".format(param_comb_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3pmPu4Xievy"
   },
   "source": [
    "## ODIN scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7R9EYh9heSp",
    "outputId": "43f817e0-b7f6-453d-daa5-1eaf070a1af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 6), dtype=float32)\n",
      "Tensor(\"truediv_6:0\", shape=(?, 6), dtype=float32)\n",
      "Tensor(\"ArgMax_12:0\", shape=(1, ?), dtype=int64)\n",
      "\n",
      "===== Temperature: 200, Magnitude: 0.0002, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [15:24<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_odin_softmax_scores_fixed(densenet_model,Validation_generator, Testing_generator_darker_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWwwGD_9W66O"
   },
   "outputs": [],
   "source": [
    "compute_odin_softmax_scores_fixed(densenet_model,Validation_generator, Testing_generator_lighter_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn-6mj2liPE0",
    "outputId": "40d0712c-c67e-4025-d589-19cb31dd76b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 6), dtype=float32)\n",
      "Tensor(\"truediv_2:0\", shape=(?, 6), dtype=float32)\n",
      "Tensor(\"ArgMax_4:0\", shape=(1, ?), dtype=int64)\n",
      "\n",
      "===== Temperature: 200, Magnitude: 0.0002, Out-Distribution =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12222/12222 [2:11:44<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_odin_softmax_scores_fixed(densenet_model, Validation_generator, Testing_generator_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3pmPu4Xievy"
   },
   "source": [
    "## Baseline scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuFiMav1xDar",
    "outputId": "4fccac5e-9c48-4d5c-f602-15f27143b14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to compute baseline softmax scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "#lighter only\n",
    "compute_baseline_softmax_scores(densenet_model, Validation_generator, Testing_generator_lighter_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzyOY-lIiiCA",
    "outputId": "f11e8e07-5915-40a8-b91b-2ddae0f86a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to compute baseline softmax scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "#darker only\n",
    "compute_baseline_softmax_scores(densenet_model, Validation_generator, Testing_generator_darker_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMXMzBJaivPi",
    "outputId": "dab6afcf-5d4c-41d1-95ba-d52a59857e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to compute baseline softmax scores\n"
     ]
    }
   ],
   "source": [
    "#ALL\n",
    "compute_baseline_softmax_scores(densenet_model, Validation_generator, Testing_generator_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZHN1-1nQd_R"
   },
   "outputs": [],
   "source": [
    "#find the ptimal threshold:\n",
    "# Concatenate the scores and create corresponding labels\n",
    "import numpy as np\n",
    "\n",
    "scores_in = np.loadtxt(\"/content/densenet121_ODIN_200_0.0002_In_Darker.txt\")\n",
    "scores_out = np.loadtxt(\"/content/densenet121_ODIN_200_0.0002_Out_Darker.txt\")\n",
    "scores_D = np.concatenate([scores_in, scores_out])\n",
    "labels_D = np.concatenate([np.zeros(len(scores_in)), np.ones(len(scores_out))])\n",
    "\n",
    "    # Use ROC curve to find the optimal threshold\n",
    "fpr, tpr, thresholds = roc_curve(labels_D, scores_D)\n",
    "optimal_threshold_D = thresholds[np.argmax(tpr - fpr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6G6zVBpUWNg",
    "outputId": "452a49d9-5be1-44eb-ad74-f95d9f0ebe6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17385952174663544\n"
     ]
    }
   ],
   "source": [
    "print(optimal_threshold) #ODIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCJXT0Ev-cgI",
    "outputId": "98ab2db5-05ea-4601-bec0-7137170d6771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9962658286094666\n"
     ]
    }
   ],
   "source": [
    "print(optimal_threshold) #NN Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tve8f5bZzung"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
